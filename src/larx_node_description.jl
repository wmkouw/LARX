using ForneyLab
using LinearAlgebra
import ForneyLab: SoftFactor, @ensureVariables, generateId, addNode!, associate!,
                  averageEnergy, differentialEntropy, Interface, Variable, slug, ProbabilityDistribution,
				  unsafeLogMean, unsafeMean, unsafeCov, unsafePrecision, unsafeMeanCov,
                  ultimatePartner, region, Region, isClamped, currentInferenceAlgorithm
export LatentAutoregressiveX, LARX

"""
Description:

    A Latent Autoregressive model with eXogenous input (LARX).

    Consider states x ‚àà R, with y = [x_k, x_k-1, ... x_k-M+1] and 
    z = [x_k-1, ... x_k-M]. The first element of y (x_k) is produced by an inner 
    product between autoregression coefficients Œ∏ and M previous states (i.e. z),
    as well as a coefficient Œ∑ times an input variable u. The remainder of y is
    produced by the first M-1 elements of z. 

    The node function is a Gaussian with mean-precision parameterization:

    f(y, Œ∏, z, Œ∑, u, Œ≥) = ùí©(y | A(Œ∏)z + B(Œ∑)u, V(Œ≥)),

    where A(Œ∏) is a state transition matrix dependent on unknown coefficients Œ∏.
    The matrix is split into a matrix 
        
        S = |0 .. 0
              I   0|, 
        
    which shifts the elements of the state vector downwards and drops the 
    oldest state x_k-M, and a vector s = [1 .. 0]', which can be used to 
    construct a matrix with the autoregression coefficients as top row; 
    
        sŒ∏' = | Œ∏_1 .. Œ∏_M
                 ‚ãÆ       ‚ãÆ
                 0  ..  0 |
        
    B(Œ∑)u is a scaled linear additive control and V(Œ≥) is a covariance 
    matrix based on process precision Œ≥.

Interfaces:

    1. y (state vector at time k; i.e. [x_k, x_k-1, ... x_k-M+1])
    2. Œ∏ (autoregression coefficients; i.e. [Œ∏_1, ... Œ∏_M])
    3. z (state vector at time k-1; i.e. [x_k-1, ... x_k-M])
    4. Œ∑ (control coefficient)
    5. u (control / input)
    6. Œ≥ (precision)

Construction:

    LatentAutoregressiveX(y, Œ∏, z, Œ∑, u, Œ≥, id=:some_id)
"""


mutable struct LatentAutoregressiveX <: SoftFactor
    id::Symbol
    interfaces::Vector{Interface}
    i::Dict{Symbol,Interface}

    function LatentAutoregressiveX(y, Œ∏, z, Œ∑, u, Œ≥; id=generateId(LatentAutoregressiveX))
        @ensureVariables(y, z, Œ∏, Œ∑, u, Œ≥)
        self = new(id, Array{Interface}(undef, 6), Dict{Symbol,Interface}())
        addNode!(currentGraph(), self)
        self.i[:y] = self.interfaces[1] = associate!(Interface(self), y)
        self.i[:z] = self.interfaces[2] = associate!(Interface(self), z)
        self.i[:Œ∏] = self.interfaces[3] = associate!(Interface(self), Œ∏)
        self.i[:Œ∑] = self.interfaces[4] = associate!(Interface(self), Œ∑)
        self.i[:u] = self.interfaces[5] = associate!(Interface(self), u)
        self.i[:Œ≥] = self.interfaces[6] = associate!(Interface(self), Œ≥)
        return self
    end
end

slug(::Type{LatentAutoregressiveX}) = "LARX"

function averageEnergy(::Type{LatentAutoregressiveX},
                       marg_y::ProbabilityDistribution{Multivariate},
                       marg_z::ProbabilityDistribution{Multivariate},
                       marg_Œ∏::ProbabilityDistribution{Multivariate},
                       marg_Œ∑::ProbabilityDistribution{Univariate},
                       marg_u::ProbabilityDistribution{Univariate},
                       marg_Œ≥::ProbabilityDistribution{Univariate})

    # Expectations of marginal beliefs
    my, Vy = unsafeMeanCov(marg_y)
    mz, Vz = unsafeMeanCov(marg_z)
    mŒ∏, VŒ∏ = unsafeMeanCov(marg_Œ∏)
    mŒ∑, vŒ∑ = unsafeMeanCov(marg_Œ∑)
    mu, vu = unsafeMeanCov(marg_u)
    mŒ≥ = unsafeMean(marg_Œ≥)

    error("Not implemented yet")

    # Compute
    Az = mŒ∏*mz
    Eg2 = Eg*Eg' + Jx'*Vz*Jx + JŒ∏'*VŒ∏*JŒ∏

    # Expand square and pre-compute terms
    sq1 = my[1]^2 + Vy[1,1]
	sq2 = my[1]*(Eg + mŒ∑*mu)
	sq3 = Eg2 + 2*Eg*mŒ∑*mu + (mŒ∑^2 + vŒ∑)*(mu + vu)

	# Compute average energy
	AE = 1/2*log(2*œÄ) -1/2*unsafeLogMean(marg_Œ≥) +1/2*mŒ≥*(sq1 -2*sq2 + sq3)

    # Correction
    AE += differentialEntropy(marg_y)
    marg_y1 = ProbabilityDistribution(Univariate, GaussianMeanVariance, m=my[1], v=Vy[1,1])
    AE -= differentialEntropy(marg_y1)

    return AE
end

